<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>10 Automatisches (Algorithmisches) Differenzieren | Numerik des Maschinellen Lernens</title>
  <meta name="description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2024" />
  <meta name="generator" content="bookdown 0.41 and GitBook 2.6.7" />

  <meta property="og:title" content="10 Automatisches (Algorithmisches) Differenzieren | Numerik des Maschinellen Lernens" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2024" />
  <meta name="github-repo" content="highlando/script-ndml" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="10 Automatisches (Algorithmisches) Differenzieren | Numerik des Maschinellen Lernens" />
  
  <meta name="twitter:description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2024" />
  

<meta name="author" content="Jan Heiland" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="best-and-universal-approximation.html"/>
<link rel="next" href="implementierungen-anwendungsbeispiele-backpropagation.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">NdML</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Vorwort</a></li>
<li class="chapter" data-level="1" data-path="einführung.html"><a href="einführung.html"><i class="fa fa-check"></i><b>1</b> Einführung</a>
<ul>
<li class="chapter" data-level="1.1" data-path="einführung.html"><a href="einführung.html#was-ist-ein-algorithmus"><i class="fa fa-check"></i><b>1.1</b> Was ist ein Algorithmus</a></li>
<li class="chapter" data-level="1.2" data-path="einführung.html"><a href="einführung.html#konsistenz-stabilität-genauigkeit"><i class="fa fa-check"></i><b>1.2</b> Konsistenz, Stabilität, Genauigkeit</a></li>
<li class="chapter" data-level="1.3" data-path="einführung.html"><a href="einführung.html#rechenkomplexität"><i class="fa fa-check"></i><b>1.3</b> Rechenkomplexität</a></li>
<li class="chapter" data-level="1.4" data-path="einführung.html"><a href="einführung.html#literatur"><i class="fa fa-check"></i><b>1.4</b> Literatur</a></li>
<li class="chapter" data-level="1.5" data-path="einführung.html"><a href="einführung.html#übungen"><i class="fa fa-check"></i><b>1.5</b> Übungen</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html"><i class="fa fa-check"></i><b>2</b> Fehler und Konditionierung</a>
<ul>
<li class="chapter" data-level="2.1" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html#fehler"><i class="fa fa-check"></i><b>2.1</b> Fehler</a></li>
<li class="chapter" data-level="2.2" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html#kondition"><i class="fa fa-check"></i><b>2.2</b> Kondition</a></li>
<li class="chapter" data-level="2.3" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html#kondition-der-grundrechenarten"><i class="fa fa-check"></i><b>2.3</b> Kondition der Grundrechenarten</a></li>
<li class="chapter" data-level="2.4" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html#übungen-1"><i class="fa fa-check"></i><b>2.4</b> Übungen</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="iterative-methoden.html"><a href="iterative-methoden.html"><i class="fa fa-check"></i><b>3</b> Iterative Methoden</a>
<ul>
<li class="chapter" data-level="3.1" data-path="iterative-methoden.html"><a href="iterative-methoden.html#iterative-methoden-als-fixpunktiteration"><i class="fa fa-check"></i><b>3.1</b> Iterative Methoden als Fixpunktiteration</a></li>
<li class="chapter" data-level="3.2" data-path="iterative-methoden.html"><a href="iterative-methoden.html#gradientenabstiegsverfahren"><i class="fa fa-check"></i><b>3.2</b> Gradientenabstiegsverfahren</a></li>
<li class="chapter" data-level="3.3" data-path="iterative-methoden.html"><a href="iterative-methoden.html#auxiliary-function-methods"><i class="fa fa-check"></i><b>3.3</b> Auxiliary Function Methods</a></li>
<li class="chapter" data-level="3.4" data-path="iterative-methoden.html"><a href="iterative-methoden.html#übungen-2"><i class="fa fa-check"></i><b>3.4</b> Übungen</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html"><i class="fa fa-check"></i><b>4</b> Stochastisches Gradientenverfahren</a>
<ul>
<li class="chapter" data-level="4.1" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html#motivation-und-algorithmus"><i class="fa fa-check"></i><b>4.1</b> Motivation und Algorithmus</a></li>
<li class="chapter" data-level="4.2" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html#iterative_method"><i class="fa fa-check"></i><b>4.2</b> Stochastisches Abstiegsverfahren</a></li>
<li class="chapter" data-level="4.3" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html#konvergenzanalyse"><i class="fa fa-check"></i><b>4.3</b> Konvergenzanalyse</a></li>
<li class="chapter" data-level="4.4" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html#übungen-3"><i class="fa fa-check"></i><b>4.4</b> Übungen</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ein-nn-beispiel.html"><a href="ein-nn-beispiel.html"><i class="fa fa-check"></i><b>5</b> Ein NN Beispiel</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ein-nn-beispiel.html"><a href="ein-nn-beispiel.html#der-penguins-datensatz"><i class="fa fa-check"></i><b>5.1</b> Der PENGUINS Datensatz</a></li>
<li class="chapter" data-level="5.2" data-path="ein-nn-beispiel.html"><a href="ein-nn-beispiel.html#ein-2-layer-neuronales-netz-zur-klassifizierung"><i class="fa fa-check"></i><b>5.2</b> Ein <em>2</em>-Layer Neuronales Netz zur Klassifizierung</a></li>
<li class="chapter" data-level="5.3" data-path="ein-nn-beispiel.html"><a href="ein-nn-beispiel.html#beispiel-implementierung"><i class="fa fa-check"></i><b>5.3</b> Beispiel Implementierung</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="singulärwert-zerlegung.html"><a href="singulärwert-zerlegung.html"><i class="fa fa-check"></i><b>6</b> Singulärwert Zerlegung</a>
<ul>
<li class="chapter" data-level="6.1" data-path="singulärwert-zerlegung.html"><a href="singulärwert-zerlegung.html#definition-und-eigenschaften"><i class="fa fa-check"></i><b>6.1</b> Definition und Eigenschaften</a></li>
<li class="chapter" data-level="6.2" data-path="singulärwert-zerlegung.html"><a href="singulärwert-zerlegung.html#numerische-berechnung"><i class="fa fa-check"></i><b>6.2</b> Numerische Berechnung</a></li>
<li class="chapter" data-level="6.3" data-path="singulärwert-zerlegung.html"><a href="singulärwert-zerlegung.html#aufgaben"><i class="fa fa-check"></i><b>6.3</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="pca-und-weitere-svd-anwendungen.html"><a href="pca-und-weitere-svd-anwendungen.html"><i class="fa fa-check"></i><b>7</b> PCA und weitere SVD Anwendungen</a>
<ul>
<li class="chapter" data-level="7.1" data-path="pca-und-weitere-svd-anwendungen.html"><a href="pca-und-weitere-svd-anwendungen.html#proper-orthogonal-decomposition-pod"><i class="fa fa-check"></i><b>7.1</b> Proper-Orthogonal Decomposition – POD</a></li>
<li class="chapter" data-level="7.2" data-path="pca-und-weitere-svd-anwendungen.html"><a href="pca-und-weitere-svd-anwendungen.html#simultane-diagonalisierung"><i class="fa fa-check"></i><b>7.2</b> Simultane Diagonalisierung</a></li>
<li class="chapter" data-level="7.3" data-path="pca-und-weitere-svd-anwendungen.html"><a href="pca-und-weitere-svd-anwendungen.html#pca"><i class="fa fa-check"></i><b>7.3</b> PCA</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>8</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="8.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#problemstellung"><i class="fa fa-check"></i><b>8.1</b> Problemstellung</a></li>
<li class="chapter" data-level="8.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximierung-des-minimalen-abstands"><i class="fa fa-check"></i><b>8.2</b> Maximierung des Minimalen Abstands</a></li>
<li class="chapter" data-level="8.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#aufgaben-1"><i class="fa fa-check"></i><b>8.3</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="best-and-universal-approximation.html"><a href="best-and-universal-approximation.html"><i class="fa fa-check"></i><b>9</b> Best and Universal Approximation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="best-and-universal-approximation.html"><a href="best-and-universal-approximation.html#universal-approximation"><i class="fa fa-check"></i><b>9.1</b> Universal Approximation</a></li>
<li class="chapter" data-level="9.2" data-path="best-and-universal-approximation.html"><a href="best-and-universal-approximation.html#aufgaben-2"><i class="fa fa-check"></i><b>9.2</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html"><i class="fa fa-check"></i><b>10</b> Automatisches (Algorithmisches) Differenzieren</a>
<ul>
<li class="chapter" data-level="10.1" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#andere-differentiationsmethoden"><i class="fa fa-check"></i><b>10.1</b> Andere Differentiationsmethoden</a></li>
<li class="chapter" data-level="10.2" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#anwendungen"><i class="fa fa-check"></i><b>10.2</b> Anwendungen</a></li>
<li class="chapter" data-level="10.3" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#vorwärts--und-rückwärtsakkumulation"><i class="fa fa-check"></i><b>10.3</b> Vorwärts- und Rückwärtsakkumulation</a></li>
<li class="chapter" data-level="10.4" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#ad-vorwärtsmodus"><i class="fa fa-check"></i><b>10.4</b> AD – Vorwärtsmodus</a></li>
<li class="chapter" data-level="10.5" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#rückwärtsmodus"><i class="fa fa-check"></i><b>10.5</b> Rückwärtsmodus</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html"><i class="fa fa-check"></i><b>11</b> Implementierungen, Anwendungsbeispiele, Backpropagation</a>
<ul>
<li class="chapter" data-level="11.1" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#exkurs-gradienten-und-repräsentation"><i class="fa fa-check"></i><b>11.1</b> Exkurs – Gradienten und Repräsentation</a></li>
<li class="chapter" data-level="11.2" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#backpropagation"><i class="fa fa-check"></i><b>11.2</b> Backpropagation</a></li>
<li class="chapter" data-level="11.3" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#praktische-berechnung-des-gradienten"><i class="fa fa-check"></i><b>11.3</b> Praktische Berechnung des Gradienten</a></li>
<li class="chapter" data-level="11.4" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#implementierungen-und-beispiele"><i class="fa fa-check"></i><b>11.4</b> Implementierungen und Beispiele</a></li>
<li class="chapter" data-level="11.5" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#aufgaben-3"><i class="fa fa-check"></i><b>11.5</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="nachklapp.html"><a href="nachklapp.html"><i class="fa fa-check"></i><b>12</b> Nachklapp</a></li>
<li class="chapter" data-level="" data-path="referenzen.html"><a href="referenzen.html"><i class="fa fa-check"></i>Referenzen</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Numerik des Maschinellen Lernens</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="automatisches-algorithmisches-differenzieren" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">10</span> Automatisches (Algorithmisches) Differenzieren<a href="automatisches-algorithmisches-differenzieren.html#automatisches-algorithmisches-differenzieren" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<!-- \newcommand\ipro[2]{\bigl \langle #1, \, #2\bigr\rangle } -->
<p>In der Mathematik und im Bereich der Computeralgebra ist das <em>automatische
Differenzieren</em> (auch <em>Auto-Differenzieren</em>, <em>Autodiff</em> oder einfach <em>AD</em>
genannt und in anderen communities als <em>algorithmisches Differenzieren</em> oder
<em>computergestütztes Differenzieren</em> bezeichnet), ein Satz von Techniken zur
Berechnung (der Werte(!)) der partiellen Ableitung einer durch ein
Computerprogramm spezifizierten Funktion.</p>
<p>AD nutzt die Tatsache, dass jede Computerberechnung,
egal wie kompliziert, eine Sequenz von elementaren arithmetischen Operationen
(Addition, Subtraktion, Multiplikation, Division usw.) und elementaren
Funktionen (exp, log, sin, cos usw.) ausführt. Durch wiederholte Anwendung der
Kettenregel auf diese Operationen können partielle Ableitungen beliebiger
Ordnung automatisch, genau bis zur Arbeitspräzision und mit höchstens einem
kleinen konstanten Faktor mehr an arithmetischen Operationen als das
ursprüngliche Programm berechnet werden.</p>
<div id="andere-differentiationsmethoden" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Andere Differentiationsmethoden<a href="automatisches-algorithmisches-differenzieren.html#andere-differentiationsmethoden" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In manchen Anwendungsfällen wird auf eine symbolische Repräsentation von
Funktionen und Variablen in der computergestützten Berechnung
zurückgegriffen. Sogenannte Computeralgebra Pakete wie <code>Maple</code>,
<code>Mathematica</code> oder die <em>Python</em> Bibliotheken <code>SageMath</code> oder <code>Sympy</code> können
dann automatisiert auch Operationen auf Funktionen wie Integralberechnung und
eben auch Differentiation <em>exakt</em> ausführen. Durch die Kettenregel und
unter der Massgabe, dass alle Codes letztlich nur elementare Funktionen mit
bekannten Ableitungen verschalten, wäre es auch möglich, bei
entsprechender Implementierung des primären Progamms, auch automatisiert
die Ableitungen zu erzeugen. Allerdings ist der Mehraufwand in der symbolischen
Programmierung erheblich und die Auswertung der Ausdrücke langsam, sodass
symbolische Berechnungen (und entsprechend auch die Möglichkeit der
symbolischen automatischen Differentiation) nur in spezifischen und insbesondere
nicht in “grossen” und “multi-purpose” Algorithmen zur Anwendung kommen.</p>
<div id="ad-vs-sd" class="JHSAYS">
<p>Auch AD rechnet symbolisch und rekursiv mit der Kettenregel. Der Unterschied ist, dass AD nur mit Funktionswerten der
Ableitungen arbeitet während die symbolische Ableitung versucht, die
Ableitung als Funktion zu erzeugen.</p>
</div>
<p>Auf der anderen Seite steht die <em>numerische Differentiation</em> (beispielsweise durch Berechnung von Differenzenquotienten).
Diese Methode ist universell (Ableitungen können ohne Kenntnis dessen
berechnet werden, was im Innern des Programms alles passiert) ist jedoch enorm schlecht konditioniert (da im
Zähler des Differenzenquotienten fast gleich grosse Größen
subtrahiert werden). Die Bestimmung einer passenden Schrittweite muss immer <em>ad
hoc</em> erfolgen und macht diese Berechnung zusätzlich teuer.</p>
<p>Sind höhere Ableitungen gefragt, verstärken sich zudem die
Komplexität (für die symbolische Berechnung) und die Fehlerverstärkung (in der numerischen Differentiation).</p>
</div>
<div id="anwendungen" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> Anwendungen<a href="automatisches-algorithmisches-differenzieren.html#anwendungen" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Automatisches Differenzieren ist ein entscheidender Baustein im Erfolg des
maschinellen Lernens. Jan könnte sagen, dass ohne AD die Optimierung der
neuronalen Netze mit tausenden bis Millionen von Parametern nicht möglich
wäre.</p>
</div>
<div id="vorwärts--und-rückwärtsakkumulation" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> Vorwärts- und Rückwärtsakkumulation<a href="automatisches-algorithmisches-differenzieren.html#vorwärts--und-rückwärtsakkumulation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="kettenregel-der-partiellen-ableitungen-zusammengesetzter-funktionen" class="section level3 hasAnchor" number="10.3.1">
<h3><span class="header-section-number">10.3.1</span> Kettenregel der partiellen Ableitungen zusammengesetzter Funktionen<a href="automatisches-algorithmisches-differenzieren.html#kettenregel-der-partiellen-ableitungen-zusammengesetzter-funktionen" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Grundlegend für die automatische Differentiation ist die Zerlegung von
Differentialen, die durch die Kettenregel der partiellen Ableitungen
zusammengesetzter Funktionen bereitgestellt wird. Für die einfache
Zusammensetzung</p>
<p><span class="math display">\[\begin{align*}
y &amp;= f(g(h(x))) = f(g(h(w_0))) = f(g(w_1)) = f(w_2) = w_3 \\
w_0 &amp;= x \\
w_1 &amp;= h(w_0) \\
w_2 &amp;= g(w_1) \\
w_3 &amp;= f(w_2) = y
\end{align*}\]</span></p>
<p>ergibt die Kettenregel für die Werte zu einem fixen Wert <span class="math inline">\(x^*\)</span> von <span class="math inline">\(x\)</span>:</p>
<p><span class="math display" id="eq:eqn-chainrule-fgh">\[\begin{equation}
\begin{split}
\frac{\partial y(x^*)}{\partial x} &amp;=
\frac{\partial y}{\partial w_2}\Biggl|_{w_2=g(h(x^*))} \frac{\partial w_2}{\partial w_1} \Biggl|_{w_1=h(x^*)}\frac{\partial w_1}{\partial w_0}\Biggl|_{w_0=x^*} \\
&amp;= \frac{\partial f}{\partial w_2}(w_2^*)\biggl[ \frac{\partial g}{\partial w_1}(w_1^*) \Bigl[\frac{\partial h}{\partial x}(w_0^*) \Bigr] \biggr]
\end{split}
\tag{10.1}
\end{equation}\]</span></p>
<p>Für multivariate Funktionen gilt die mehrdimensionale Kettenregel und das
Produkt der Ableitungen wird zum Produkt der Jacobi-Matrizen.</p>
</div>
</div>
<div id="ad-vorwärtsmodus" class="section level2 hasAnchor" number="10.4">
<h2><span class="header-section-number">10.4</span> AD – Vorwärtsmodus<a href="automatisches-algorithmisches-differenzieren.html#ad-vorwärtsmodus" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Im sogenannten <em>Vorwärtsmodus</em> (auch <em>forward accumulation</em>) wird jede Teilfunktion im Programm
so erweitert, dass mit dem Funktionswert direkt der Wert der Ableitung
mitgeliefert wird. Ein Programmfluss für obiges <span class="math inline">\(f\)</span>, <span class="math inline">\(g\)</span>, <span class="math inline">\(h\)</span> Beispiel
würde also jeweils immer zwei Berechnungen machen und die Ableitung
<em>akkumulieren</em>:</p>
<table>
<colgroup>
<col width="18%" />
<col width="37%" />
<col width="18%" />
<col width="25%" />
</colgroup>
<thead>
<tr class="header">
<th>Schritt</th>
<th>Funktionswert</th>
<th>Ableitung</th>
<th>Akkumulation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>0</code></td>
<td><span class="math inline">\(w_0 = x\)</span></td>
<td><span class="math inline">\(\dot w_0 = 1\)</span></td>
<td><span class="math inline">\(1\)</span></td>
</tr>
<tr class="even">
<td><code>1</code></td>
<td><span class="math inline">\((w_1, \dot w_1) = (h(w_0), h&#39;(w_0))\)</span></td>
<td><span class="math inline">\(\dot w_1\)</span></td>
<td><span class="math inline">\(\dot w_1 \cdot 1\)</span></td>
</tr>
<tr class="odd">
<td><code>2</code></td>
<td><span class="math inline">\((w_2, \dot w_2) = (g(w_1), g(w_1))\)</span></td>
<td><span class="math inline">\(\dot w_2\)</span></td>
<td><span class="math inline">\(\dot w_2 \cdot\dot w_1 \cdot 1\)</span></td>
</tr>
<tr class="even">
<td><code>3</code></td>
<td><span class="math inline">\((w_3, \dot w_3) = (f(w_1), f(w_1))\)</span></td>
<td><span class="math inline">\(\dot w_3\)</span></td>
<td><span class="math inline">\(\dot w_3 \cdot\dot w_2 \cdot\dot w_1 \cdot 1\)</span></td>
</tr>
</tbody>
</table>
<p>Das ergibt die Ableitung als den finalen Wert der Akkumulation. Wir bemerken,
dass hierbei</p>
<ul>
<li>die Ableitung entlang des Programmflusses immer direkt mitbestimmt wird
(deshalb <em>vorwärts</em> Modus)</li>
<li>es genügt im Schritt <span class="math inline">\(k\)</span>, den Wert <span class="math inline">\(w_{k-1}\)</span> und die Akkumulation zu
kennen.</li>
<li>Für eine Funktion <span class="math inline">\(F\otimes G \otimes h \colon \mathbb R^{}\to \mathbb R^{m}\)</span>, funktioniert die Berechnung ganz analog mit beispielsweise
<span class="math display">\[\begin{equation*}
(G, \partial G) \colon \mathbb R^{} \to \mathbb R^{\ell} \times  \mathbb R^{\ell}
\end{equation*}\]</span>
und
<span class="math display">\[\begin{equation*}
(F, \partial F) \colon \mathbb R^{\ell} \to \mathbb R^{m} \times  \mathbb
R^{m \times \ell}
\end{equation*}\]</span>
und der Akkumulation
<span class="math display">\[\begin{equation*}
\frac{\partial y}{\partial x}(x^*) = \partial F(w_2)\cdot \partial G(w_1)\cdot h&#39;(w_0) \cdot 1.
\end{equation*}\]</span></li>
<li>Für eine Funktion in mehreren Variablen, d.h. von <span class="math inline">\(\mathbb R^{n}\)</span> nach <span class="math inline">\(\mathbb R^{m}\)</span>, werden die
partiellen Ableitungen <span class="math inline">\(\frac{\partial y}{\partial x_k}\)</span> separat in eigenen Durchläfen berechnet:
<ul>
<li>damit ist an der Implementierung nichts zu ändern, nur die
Akkumulation wird mit <span class="math inline">\(\dot w_0 = \begin{bmatrix} 0 &amp; \dotsm &amp; 1 &amp; 0 &amp; \dotsm &amp; 0 \end{bmatrix}^T\)</span> initialisiert</li>
<li>eine simultane Berechnung aller Ableitungen verursacht vergleichsweise
viel <em>overhead</em> (entweder müssen die verschiedenen Richtungen im
Code
organisiert werden oder es müssen alle Zwischenberechnungen der
Ableitung gespeichert werden).</li>
</ul></li>
</ul>
<div id="fwd-ad-vs-bwd" class="JHSAYS">
<p>Wir halten fest, dass der <em>Vorwärtsmodus</em> gut funktioniert für skalare
Eingänge unabhängig von der Zahl der Ausgänge. Wir werden lernen,
dass sich beim Rückwärtmodus dieses Verhältnis umdreht. Damit:</p>
<ul>
<li><em>Vorwärtsakkumulation</em>: Bevorzugt für Funktionen <span class="math inline">\(f \colon \mathbb{R}^n \rightarrow \mathbb{R}^m\)</span>, wobei <span class="math inline">\(n \ll m\)</span>.</li>
<li><em>Rückwärtsakkumulation</em>: Bevorzugt für Funktionen <span class="math inline">\(f\colon \mathbb{R}^n \rightarrow \mathbb{R}^m\)</span>, wobei <span class="math inline">\(n \gg m\)</span>.</li>
</ul>
<p>Vor allem für neuronale Netze mit vielen Parametern und dem Fehler als
(skalaren) Ausgang, ist der Rückwärtsmodus fraglos die Methode der
Wahl. Hier wird dann typischerweise von <em>backpropagation</em> gesprochen, was eine
Adaption der Methode an die Architektur typischer neuronaler Netze ist.</p>
</div>
<p>Bevor wir zum Rückwärtsmodus kommen, noch einige Praktische
Bemerkungen anhand eines konkreten Beispiel.</p>
<p>Zunächst mal die Bemerkung, dass es vorteilhaft ist, ein Programm als ein Graph der
die Abhängigkeiten der Variablen enthält, darzustellen. Dann werden
insbesondere “nicht vorhandene” Abhängigkeiten vermieden und Speicher– und
Rechenaufwand reduziert.</p>
<p>Entsprechend werden die Zwischenwerte <span class="math inline">\(w_i\)</span> nicht mehr einfach durchnummeriert,
sondern es wird von Vorgängern gesprochen</p>
<blockquote>
<p><span class="math inline">\(w_j\)</span> ist ein <em>Vorgänger</em> von <span class="math inline">\(w_i\)</span> genau dann wenn <span class="math inline">\(w_i\)</span> unmittelbar
(also <em>explizit</em>) von <span class="math inline">\(w_j\)</span> abhängt.</p>
</blockquote>
<p>Damit (und mit Anwendung der Kettenregel im mehrdimensionalen) wird aus
<span class="math inline">\(\dot w_i = \frac{\partial w_i}{\partial w_{&quot;i-1&quot;}}\)</span> der Ausdruck</p>
<p><span class="math display">\[
\dot w_i = \sum_{j \in \{\text{Vorgänger von i}\}} \frac{\partial w_i}{\partial w_j} \dot w_j
\]</span></p>
<p>Für das Beispiel
<span class="math display">\[\begin{align*}
y
&amp;= f(x_1, x_2) \\
&amp;= x_1 x_2 + \sin x_1 \\
&amp;=: w_1 w_2 + \sin w_1 \\
&amp;=: w_3 + w_4 \\
&amp;=: w_5
\end{align*}\]</span>
ergibt sich folgender Berechnungsgraph und Fluss in der
Vorwärtsakkumulation.</p>
<!-- ![Beispiel für Vorwärtsakkumulation mit Berechnungsgraph](bilder/ForwardAccumulationAutomaticDifferentiation.png) -->
<div class="figure"><span style="display:block;" id="fig:fig-10-ad-fwd"></span>
<img src="bilder/ForwardAccumulationAutomaticDifferentiation.png" alt="Beispiel für Vorwärtsakkumulation mit Berechnungsgraph" width="100%" />
<p class="caption">
Figure 10.1: Beispiel für Vorwärtsakkumulation mit Berechnungsgraph
</p>
</div>
<p>Mit den folgenden Schritten ergibt sich für die partielle
Ableitung von <span class="math inline">\(y\)</span> bezüglich <span class="math inline">\(x_1\)</span></p>
<table>
<colgroup>
<col width="11%" />
<col width="19%" />
<col width="23%" />
<col width="46%" />
</colgroup>
<thead>
<tr class="header">
<th>Schritt</th>
<th>Funktionswert</th>
<th>Ableitung</th>
<th>Akkumulation</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>0a</code></td>
<td><span class="math inline">\(w_1=x_1\)</span></td>
<td></td>
<td><span class="math inline">\(\dot w_1 = 1\)</span></td>
</tr>
<tr class="even">
<td><code>0b</code></td>
<td><span class="math inline">\(w_2=x_2\)</span></td>
<td></td>
<td><span class="math inline">\(\dot w_2 = 0\)</span></td>
</tr>
<tr class="odd">
<td><code>1a</code></td>
<td><span class="math inline">\(w_3 = w_1w_2\)</span></td>
<td><span class="math inline">\(\partial w_3 = \begin{bmatrix} w_2&amp;w_1\end{bmatrix}\)</span></td>
<td><span class="math inline">\(\dot w_3 = \begin{bmatrix}w_2&amp;w_1\end{bmatrix}\begin{bmatrix} 1 \\ 0 \end{bmatrix} = x_2\)</span></td>
</tr>
<tr class="even">
<td><code>1b</code></td>
<td><span class="math inline">\(w_4 = \sin(w_1)\)</span></td>
<td><span class="math inline">\(\partial w_4=\cos(w_1)\)</span></td>
<td><span class="math inline">\(\dot w_4 =\cos(w_1)\cdot\dot w_1 = \cos(x_1)\)</span></td>
</tr>
<tr class="odd">
<td><code>2</code></td>
<td><span class="math inline">\(w_5 = w_3 + w_4\)</span></td>
<td><span class="math inline">\(\partial w_5=\begin{bmatrix}1 &amp; 1\end{bmatrix}\)</span></td>
<td><span class="math inline">\(\dot w_5 = \begin{bmatrix}1 &amp; 1\end{bmatrix}\begin{bmatrix} \dot w_3 \\ \dot w_4 \end{bmatrix} = x_2 + \cos(x_1)\)</span></td>
</tr>
</tbody>
</table>
</div>
<div id="rückwärtsmodus" class="section level2 hasAnchor" number="10.5">
<h2><span class="header-section-number">10.5</span> Rückwärtsmodus<a href="automatisches-algorithmisches-differenzieren.html#rückwärtsmodus" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bei der Rückwärtsakkumulation werden die sogenannten <em>Adjungierten</em>
<span class="math display">\[\begin{equation*}
\bar{w}_i = \frac{\partial y}{\partial w_i}
\end{equation*}\]</span>
berechnet. Jan beachte, dass immer <span class="math inline">\(y\)</span> <em>differenziert</em> wird und dass der
gewünschte Ausdruck bei beispielsweise <span class="math inline">\(\bar x = \frac{\partial y}{\partial x}\)</span> erreicht ist.</p>
<p>Unter Verwendung der Kettenregel ergibt sich die folgende rekursive Formel aus dem
Berechnungsgraphen:</p>
<p><span class="math display">\[\bar{w}_i = \sum_{j \in \{\text{Nachfolger von i}\}} \bar{w}_j \frac{\partial w_j}{\partial w_i}\]</span>
Sind also <em>spätere</em> Adjungierte <span class="math inline">\(\bar w_j\)</span> bekannt, können
<em>frühere</em> <span class="math inline">\(\bar w_i\)</span> bestimmt werden.</p>
<p>Die Rückwärtsakkumulation durchläuft die Kettenregel (wie in Gleichung
<a href="automatisches-algorithmisches-differenzieren.html#eq:eqn-chainrule-fgh">(10.1)</a> von außen nach
innen oder im Falle des Berechnungsgraphen (wie in Abbildung <a href="automatisches-algorithmisches-differenzieren.html#fig:fig-10-ad-bw">10.2</a> illustriert) von oben nach unten.</p>
<!-- ![Beispiel für Rückwärtsakkumulation mit Berechnungsgraph](bilder/ReverseaccumulationAD.png){#fig-10-ad-bw width="100%"} -->
<div class="figure"><span style="display:block;" id="fig:fig-10-ad-bw"></span>
<img src="bilder/ReverseaccumulationAD.png" alt="Beispiel für Rückwärtsakkumulation mit Berechnungsgraph" width="100%" />
<p class="caption">
Figure 10.2: Beispiel für Rückwärtsakkumulation mit Berechnungsgraph
</p>
</div>
<p>Die Operationen zur Berechnung der Ableitung mittels Rückwärtsakkumulation sind in der folgenden Tabelle dargestellt (beachte die umgekehrte Reihenfolge):</p>
<table>
<colgroup>
<col width="11%" />
<col width="19%" />
<col width="23%" />
<col width="46%" />
</colgroup>
<thead>
<tr class="header">
<th>Schritt</th>
<th>Funktionswert</th>
<th>Ableitung</th>
<th>Adjungierte</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>0</code></td>
<td><span class="math inline">\(y=w_5\)</span></td>
<td><span class="math inline">\(\partial y=1\)</span></td>
<td><span class="math inline">\(\bar w_5 = 1\)</span></td>
</tr>
<tr class="even">
<td><code>1</code></td>
<td><span class="math inline">\(w_5=w_4+w_3\)</span></td>
<td><span class="math inline">\(\partial w_5=\begin{bmatrix} 1&amp;1 \end{bmatrix}\)</span></td>
<td><span class="math inline">\(\bar w_5 = 1\)</span></td>
</tr>
<tr class="odd">
<td><code>2a</code></td>
<td><span class="math inline">\(w_4=\sin(w_1)\)</span></td>
<td><span class="math inline">\(\partial w_4=\cos(w_1)\)</span></td>
<td><span class="math inline">\(\bar w_4 = \bar w_5 \cdot 1 = 1\)</span></td>
</tr>
<tr class="even">
<td><code>2b</code></td>
<td><span class="math inline">\(w_3=w_1w_2\)</span></td>
<td><span class="math inline">\(\partial w_3=\begin{bmatrix}w_2&amp;w_1\end{bmatrix}\)</span></td>
<td><span class="math inline">\(\bar w_3 = \bar w_5 \cdot 1 = 1\)</span></td>
</tr>
<tr class="odd">
<td><code>3b</code></td>
<td><span class="math inline">\(w_2=x_2\)</span></td>
<td></td>
<td><span class="math inline">\(\bar w_2 = \bar w_3 w_1= x_1\)</span></td>
</tr>
<tr class="even">
<td><code>3a</code></td>
<td><span class="math inline">\(w_1=x_1\)</span></td>
<td></td>
<td><span class="math inline">\(\bar w_1 = \bar w_3 w_2 +\bar w_4 \cos(w_1) = x_2 + \cos(x_1)\)</span></td>
</tr>
</tbody>
</table>
<p>Wir bemerken, dass in einem Durchlauf, beide partiellen Ableitungen
<span class="math display">\[\begin{equation*}
\bar w_2 = \frac{\partial y}{\partial w_2}= \frac{\partial y}{\partial x_2},\quad \bar w_1 = \frac{\partial y}{\partial w_1}= \frac{\partial y}{\partial x_1}
\end{equation*}\]</span>
direkt
berechnet werden. Allerdings müssen zunächst in einem
Vorwärsdurchlauf die Gradienten <span class="math inline">\(\partial w_i\)</span> berechnet und gespeichert werden.
Das kann bei komplexen Programmen durchaus ein Nachteil sein. Ein Ausweg bietet
<em>checkpointing</em> wo nur wenige Zwischenetappen der Werte <span class="math inline">\(w_i\)</span> gespeichert
werden, aus denen bei Bedarf die Nachfolger und Gradienten zwischen den
Checkpoints erzeugt werden können.</p>
<p>Zusätzlich muss der Programmablauf (also welche Variablen aus welchen
hervorgegangen sind – die sogenannte <em>Wengert Liste</em>) verfügbar sein.</p>
<p>Hätte <span class="math inline">\(y\)</span> mehrere Komponenten, müsste
für jede Komponente die entsprechenden adjungierten in einem neuen
Durchlauf berechnet werden.</p>
<p>Andersherum ist es beim Vorwärtsmodus, bei welchem mehrere Komponenten im
Ausgang direkt berechnet werden aber für jede Eingangsvariable die
Akkumulation separat durchgeführt werden muss.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="best-and-universal-approximation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="implementierungen-anwendungsbeispiele-backpropagation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["NdML.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
