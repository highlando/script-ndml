<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11 Implementierungen, Anwendungsbeispiele, Backpropagation | Numerik des Maschinellen Lernens</title>
  <meta name="description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2024" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="11 Implementierungen, Anwendungsbeispiele, Backpropagation | Numerik des Maschinellen Lernens" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2024" />
  <meta name="github-repo" content="highlando/script-ndml" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11 Implementierungen, Anwendungsbeispiele, Backpropagation | Numerik des Maschinellen Lernens" />
  
  <meta name="twitter:description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2024" />
  

<meta name="author" content="Jan Heiland" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="automatisches-algorithmisches-differenzieren.html"/>
<link rel="next" href="nachklapp.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">NdML</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Vorwort</a></li>
<li class="chapter" data-level="1" data-path="einführung.html"><a href="einführung.html"><i class="fa fa-check"></i><b>1</b> Einführung</a>
<ul>
<li class="chapter" data-level="1.1" data-path="einführung.html"><a href="einführung.html#was-ist-ein-algorithmus"><i class="fa fa-check"></i><b>1.1</b> Was ist ein Algorithmus</a></li>
<li class="chapter" data-level="1.2" data-path="einführung.html"><a href="einführung.html#konsistenz-stabilität-genauigkeit"><i class="fa fa-check"></i><b>1.2</b> Konsistenz, Stabilität, Genauigkeit</a></li>
<li class="chapter" data-level="1.3" data-path="einführung.html"><a href="einführung.html#rechenkomplexität"><i class="fa fa-check"></i><b>1.3</b> Rechenkomplexität</a></li>
<li class="chapter" data-level="1.4" data-path="einführung.html"><a href="einführung.html#literatur"><i class="fa fa-check"></i><b>1.4</b> Literatur</a></li>
<li class="chapter" data-level="1.5" data-path="einführung.html"><a href="einführung.html#übungen"><i class="fa fa-check"></i><b>1.5</b> Übungen</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html"><i class="fa fa-check"></i><b>2</b> Fehler und Konditionierung</a>
<ul>
<li class="chapter" data-level="2.1" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html#fehler"><i class="fa fa-check"></i><b>2.1</b> Fehler</a></li>
<li class="chapter" data-level="2.2" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html#kondition"><i class="fa fa-check"></i><b>2.2</b> Kondition</a></li>
<li class="chapter" data-level="2.3" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html#kondition-der-grundrechenarten"><i class="fa fa-check"></i><b>2.3</b> Kondition der Grundrechenarten</a></li>
<li class="chapter" data-level="2.4" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html#übungen-1"><i class="fa fa-check"></i><b>2.4</b> Übungen</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="iterative-methoden.html"><a href="iterative-methoden.html"><i class="fa fa-check"></i><b>3</b> Iterative Methoden</a>
<ul>
<li class="chapter" data-level="3.1" data-path="iterative-methoden.html"><a href="iterative-methoden.html#iterative-methoden-als-fixpunktiteration"><i class="fa fa-check"></i><b>3.1</b> Iterative Methoden als Fixpunktiteration</a></li>
<li class="chapter" data-level="3.2" data-path="iterative-methoden.html"><a href="iterative-methoden.html#gradientenabstiegsverfahren"><i class="fa fa-check"></i><b>3.2</b> Gradientenabstiegsverfahren</a></li>
<li class="chapter" data-level="3.3" data-path="iterative-methoden.html"><a href="iterative-methoden.html#auxiliary-function-methods"><i class="fa fa-check"></i><b>3.3</b> Auxiliary Function Methods</a></li>
<li class="chapter" data-level="3.4" data-path="iterative-methoden.html"><a href="iterative-methoden.html#übungen-2"><i class="fa fa-check"></i><b>3.4</b> Übungen</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html"><i class="fa fa-check"></i><b>4</b> Stochastisches Gradientenverfahren</a>
<ul>
<li class="chapter" data-level="4.1" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html#motivation-und-algorithmus"><i class="fa fa-check"></i><b>4.1</b> Motivation und Algorithmus</a></li>
<li class="chapter" data-level="4.2" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html#iterative_method"><i class="fa fa-check"></i><b>4.2</b> Stochastisches Abstiegsverfahren</a></li>
<li class="chapter" data-level="4.3" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html#konvergenzanalyse"><i class="fa fa-check"></i><b>4.3</b> Konvergenzanalyse</a></li>
<li class="chapter" data-level="4.4" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html#übungen-3"><i class="fa fa-check"></i><b>4.4</b> Übungen</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ein-nn-beispiel.html"><a href="ein-nn-beispiel.html"><i class="fa fa-check"></i><b>5</b> Ein NN Beispiel</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ein-nn-beispiel.html"><a href="ein-nn-beispiel.html#der-penguins-datensatz"><i class="fa fa-check"></i><b>5.1</b> Der PENGUINS Datensatz</a></li>
<li class="chapter" data-level="5.2" data-path="ein-nn-beispiel.html"><a href="ein-nn-beispiel.html#ein-2-layer-neuronales-netz-zur-klassifizierung"><i class="fa fa-check"></i><b>5.2</b> Ein <em>2</em>-Layer Neuronales Netz zur Klassifizierung</a></li>
<li class="chapter" data-level="5.3" data-path="ein-nn-beispiel.html"><a href="ein-nn-beispiel.html#beispiel-implementierung"><i class="fa fa-check"></i><b>5.3</b> Beispiel Implementierung</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="singulärwert-zerlegung.html"><a href="singulärwert-zerlegung.html"><i class="fa fa-check"></i><b>6</b> Singulärwert Zerlegung</a>
<ul>
<li class="chapter" data-level="6.1" data-path="singulärwert-zerlegung.html"><a href="singulärwert-zerlegung.html#definition-und-eigenschaften"><i class="fa fa-check"></i><b>6.1</b> Definition und Eigenschaften</a></li>
<li class="chapter" data-level="6.2" data-path="singulärwert-zerlegung.html"><a href="singulärwert-zerlegung.html#numerische-berechnung"><i class="fa fa-check"></i><b>6.2</b> Numerische Berechnung</a></li>
<li class="chapter" data-level="6.3" data-path="singulärwert-zerlegung.html"><a href="singulärwert-zerlegung.html#aufgaben"><i class="fa fa-check"></i><b>6.3</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="pca-und-weitere-svd-anwendungen.html"><a href="pca-und-weitere-svd-anwendungen.html"><i class="fa fa-check"></i><b>7</b> PCA und weitere SVD Anwendungen</a>
<ul>
<li class="chapter" data-level="7.1" data-path="pca-und-weitere-svd-anwendungen.html"><a href="pca-und-weitere-svd-anwendungen.html#proper-orthogonal-decomposition-pod"><i class="fa fa-check"></i><b>7.1</b> Proper-Orthogonal Decomposition – POD</a></li>
<li class="chapter" data-level="7.2" data-path="pca-und-weitere-svd-anwendungen.html"><a href="pca-und-weitere-svd-anwendungen.html#simultane-diagonalisierung"><i class="fa fa-check"></i><b>7.2</b> Simultane Diagonalisierung</a></li>
<li class="chapter" data-level="7.3" data-path="pca-und-weitere-svd-anwendungen.html"><a href="pca-und-weitere-svd-anwendungen.html#pca"><i class="fa fa-check"></i><b>7.3</b> PCA</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>8</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="8.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#problemstellung"><i class="fa fa-check"></i><b>8.1</b> Problemstellung</a></li>
<li class="chapter" data-level="8.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximierung-des-minimalen-abstands"><i class="fa fa-check"></i><b>8.2</b> Maximierung des Minimalen Abstands</a></li>
<li class="chapter" data-level="8.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#aufgaben-1"><i class="fa fa-check"></i><b>8.3</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="best-and-universal-approximation.html"><a href="best-and-universal-approximation.html"><i class="fa fa-check"></i><b>9</b> Best and Universal Approximation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="best-and-universal-approximation.html"><a href="best-and-universal-approximation.html#universal-approximation"><i class="fa fa-check"></i><b>9.1</b> Universal Approximation</a></li>
<li class="chapter" data-level="9.2" data-path="best-and-universal-approximation.html"><a href="best-and-universal-approximation.html#aufgaben-2"><i class="fa fa-check"></i><b>9.2</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html"><i class="fa fa-check"></i><b>10</b> Automatisches (Algorithmisches) Differenzieren</a>
<ul>
<li class="chapter" data-level="10.1" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#andere-differentiationsmethoden"><i class="fa fa-check"></i><b>10.1</b> Andere Differentiationsmethoden</a></li>
<li class="chapter" data-level="10.2" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#anwendungen"><i class="fa fa-check"></i><b>10.2</b> Anwendungen</a></li>
<li class="chapter" data-level="10.3" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#vorwärts--und-rückwärtsakkumulation"><i class="fa fa-check"></i><b>10.3</b> Vorwärts- und Rückwärtsakkumulation</a></li>
<li class="chapter" data-level="10.4" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#ad-vorwärtsmodus"><i class="fa fa-check"></i><b>10.4</b> AD – Vorwärtsmodus</a></li>
<li class="chapter" data-level="10.5" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#rückwärtsmodus"><i class="fa fa-check"></i><b>10.5</b> Rückwärtsmodus</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html"><i class="fa fa-check"></i><b>11</b> Implementierungen, Anwendungsbeispiele, Backpropagation</a>
<ul>
<li class="chapter" data-level="11.1" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#exkurs-gradienten-und-repräsentation"><i class="fa fa-check"></i><b>11.1</b> Exkurs – Gradienten und Repräsentation</a></li>
<li class="chapter" data-level="11.2" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#backpropagation"><i class="fa fa-check"></i><b>11.2</b> Backpropagation</a></li>
<li class="chapter" data-level="11.3" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#praktische-berechnung-des-gradienten"><i class="fa fa-check"></i><b>11.3</b> Praktische Berechnung des Gradienten</a></li>
<li class="chapter" data-level="11.4" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#implementierungen-und-beispiele"><i class="fa fa-check"></i><b>11.4</b> Implementierungen und Beispiele</a></li>
<li class="chapter" data-level="11.5" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#aufgaben-3"><i class="fa fa-check"></i><b>11.5</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="nachklapp.html"><a href="nachklapp.html"><i class="fa fa-check"></i><b>12</b> Nachklapp</a></li>
<li class="chapter" data-level="" data-path="referenzen.html"><a href="referenzen.html"><i class="fa fa-check"></i>Referenzen</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Numerik des Maschinellen Lernens</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="implementierungen-anwendungsbeispiele-backpropagation" class="section level1 hasAnchor" number="11">
<h1><span class="header-section-number">11</span> Implementierungen, Anwendungsbeispiele, Backpropagation<a href="implementierungen-anwendungsbeispiele-backpropagation.html#implementierungen-anwendungsbeispiele-backpropagation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="exkurs-gradienten-und-repräsentation" class="section level2 hasAnchor" number="11.1">
<h2><span class="header-section-number">11.1</span> Exkurs – Gradienten und Repräsentation<a href="implementierungen-anwendungsbeispiele-backpropagation.html#exkurs-gradienten-und-repräsentation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Die Berechnung von Gradienten ist ebenso wesentlich wie schwierig in numerischen
Algorithmen.
Viele <em>erfolgreiche</em> Algorithmen basieren auf effizient berechenbaren
Darstellungen oder Approximationen des Gradienten.</p>
<p>Ein erstes Beispiel ist der stochastische Gradientenabstieg, der auf einen
Schätzer statt des eigentlichen Gradienten baut.</p>
<p>In der Optimierung mit (partiellen) Differentialgleichungen wie
<span class="math display">\[\begin{equation*}
\mathcal J(u) = \frac 12 \int_0^T\|x(s)-x^*\|^2 + \|u(s)\|^2\,\mathsf{d}s\to \min_{u} \quad \text{s.t. }\dot x(t) =
Ax(t) + Bu(t)
\end{equation*}\]</span>
macht es einen entscheidenden Unterschied, dass der Gradient
<span class="math display">\[\partial_u \mathcal J(u) = u + B^Tp \]</span>
(Jan beachte, dass <span class="math inline">\(u\)</span> eine Funktion ist, also ein <span class="math inline">\(\infty\)</span>-dimensionales
Objekt) über die Lösung der adjungierten Gleichung
<span class="math display">\[\begin{equation*}
-\dot p(t) = A^T p(t) + x(t)-x^*, \quad p(T) = 0
\end{equation*}\]</span>
definiert ist (siehe beispielsweise <span class="citation">(Kurdila and Zabarankin <a href="#ref-KurZ05" role="doc-biblioref">2005</a>, Theorem 5.5.1)</span> für ein
abstraktes Resultat und bspw. <span class="citation">(Tröltzsch <a href="#ref-Tr09" role="doc-biblioref">2009</a>, Abschnitt 5.9.1)</span> für eine
Umsetzung in einem Gradientenabstiegsverfahren mit PDEs).</p>
<p>Während diese Resultate in der Theorie die Wohlgestelltheit der
Optimierungsprobleme helfen zu analysieren, werden Sie in der Praxis gerne
verwendet weil die Lösung einer Differentialgleichung einfacher umzusetzen
ist (und im Zweifel auch effizienter) als die Berechnung von sehr abstrakten
Gradienten.</p>
<p>Um abstraktere Gradienten zu charakterisieren hilft oftmals die Definition der
<em>totalen Ableitung</em> einer Funktion <span class="math inline">\(f\colon X\to Y\)</span> bei einem <span class="math inline">\(x\in X\)</span> als die lineare Abbildung <span class="math inline">\(L(x)\colon X\to Y\)</span> derart, dass
<span class="math display">\[\begin{equation*}
f(x+h) - f(x) - L(x)[h] = o(\| h\|)
\end{equation*}\]</span>
für <span class="math inline">\(h\in X\)</span> gilt.</p>
<p>Seien beispielsweise die Räume als <span class="math inline">\(X=Y=\mathbb R^{n\times n}\)</span> gegeben und
<span class="math display" id="eq:eqn-ric-operator">\[\begin{equation}
f(S) = A^TS + SA -SRS + Q
\tag{11.1}
\end{equation}\]</span>
dann hat der <em>Riccati</em> Operator <span class="math inline">\(f\)</span> mit Koeffizienten <span class="math inline">\(A\)</span>, <span class="math inline">\(R\)</span>, <span class="math inline">\(Q \in \mathbb R^{n\times n}\)</span>
demnach die Realisierung des Gradienten (an der Stelle <span class="math inline">\(S_0\)</span>) gegeben als
<span class="math display">\[\begin{equation*}
L(S_0)[h] = (A^T-S_0R)h + h(A-RS_0).
\end{equation*}\]</span></p>
<p>Für neuronale Netze ist der Gradient von <span class="math inline">\(f(x; \tilde A, b)=\tilde Ax+b\)</span> bezüglich der
<em>Gewichte</em> <span class="math inline">\(\tilde A\in \mathbb R^{m\times n-1}\)</span> und <span class="math inline">\(b\in \mathbb R^{m}\)</span> interessant.
Zunächst mal verstehen wir die affine lineare Abbildung im
projektiven Raum vermöge
<span class="math display">\[\begin{equation*}
y = Ax + b \longleftrightarrow \begin{bmatrix} y \\ 1 \end{bmatrix} = \begin{bmatrix}A &amp; b \\ 0 &amp; 1 \end{bmatrix}\begin{bmatrix} x \\ 1
\end{bmatrix}
\end{equation*}\]</span>
und betrachten einfach <span class="math inline">\(f(x; A)=Ax\)</span> mit <span class="math inline">\(A\in \mathbb R^{m\times n}\)</span>. Gemäß der Ableitungsformel gilt
<span class="math display">\[\begin{equation*}
f(x;A+h) - f(x;A) - hx = (A+h)x - Ax - hx = 0 = o(\|h\|)
\end{equation*}\]</span>
also im Prinzip ist <span class="math inline">\(L(A)\leftrightarrow x\)</span> die Ableitung wenn auch nicht als Vektor sondern als
<span class="math display">\[\begin{equation*}
L_{[x]}\colon \mathbb R^{n\times m} \to \mathbb R^{n}\colon h\to L_{[x]} h = hx.
\end{equation*}\]</span></p>
<div id="Lx-not-LA" class="JHSAYS">
<p>Weil hier an der Stelle <span class="math inline">\(A\)</span> abgeleitet wird, sollte hier <span class="math inline">\(L(A)\)</span> stehen. Da bei
linearen Abbildung die Ableitung überall gleich ist, gilt <span class="math inline">\(L(A)\equiv L\)</span>
unabhängig von <span class="math inline">\(A\)</span>. Für später wird es interessant sein, dass
die Ableitung direkt mit <span class="math inline">\(x\)</span> zusammenhängt, sodass wir das <span class="math inline">\(x\)</span> in die
Definition mit aufnehmen.</p>
</div>
<p>Diese Abbildung <span class="math inline">\(L_{[x]}\)</span> ist linear und könnte als Matrix (bzw. Tensor im Sinne
einer höherdimensionalen Datenstruktur)
im <span class="math inline">\(\mathbb R^{n \times (n\times m)}\)</span> realisiert werden.</p>
</div>
<div id="backpropagation" class="section level2 hasAnchor" number="11.2">
<h2><span class="header-section-number">11.2</span> Backpropagation<a href="implementierungen-anwendungsbeispiele-backpropagation.html#backpropagation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Die besondere Architektur neuronaler Netze, wie sie im Machine Learning
verwendet werden, impliziert eine besondere Struktur in der
Gradientenberechnung.</p>
<p>Wir betrachten ein 2-layer Netzwerk
<span class="math display">\[\begin{equation*}
s(x) = \sigma(A_2 \sigma(A_1x))
\end{equation*}\]</span>
mit den <em>bias</em>-Vektoren <span class="math inline">\(b_1\)</span> und <span class="math inline">\(b_2\)</span> wie oben erläutert bereits in den Matrizen <span class="math inline">\(A_1\in \mathbb R^{n_0\times n_1}\)</span> und <span class="math inline">\(A_2 \in \mathbb R^{n_2\times n_1}\)</span> integriert.
Für das <em>learning</em> wird zu Datenpunkten <span class="math inline">\((x, y) \in \mathbb R^{n_0}\times \mathbb R^{n_2}\)</span> die Ableitung der Funktion
<span class="math display">\[\begin{equation*}
(A_1, A_2) \to l((x,y); A_1, A_2) = \|y-s(x)\|_2^2
\end{equation*}\]</span>
bezüglich des Parameter“vektors” <span class="math inline">\(p=(A_2, A_1)\)</span> gesucht.</p>
<p>Eine erste Anwendung der Kettenregel ergibt
<span class="math display">\[\begin{equation*}
\partial_p l((x,y); p) = -2\bigl \langle y-s(x;p), \, \partial_p s(x;p)\bigr\rangle 
\end{equation*}\]</span>
sodass wir uns erstmal um die Ableitung von <span class="math inline">\(s\)</span> bezüglich <span class="math inline">\(p\)</span> kümmern
können.</p>
<p>Bezüglich <span class="math inline">\(A_2\)</span> ergibt die Kettenregel direkt
<span class="math display">\[\begin{equation*}
\partial_{A_2} = \sigma&#39;(A_2x_1)L_{[x_1]}
\end{equation*}\]</span>
wobei</p>
<ul>
<li><span class="math inline">\(x_1:=\sigma(A_1x)\)</span> der Zwischenwert nach der vorletzten (hier der ersten) <em>layer</em> ist,</li>
<li><span class="math inline">\(\sigma&#39;(A_2x_1)\in \mathbb R^{n_2&#39;\times n_2}\)</span> die (diagonale) Jacobimatrix der komponentenweise definierten Aktivierungsfunktion <span class="math inline">\(\sigma\)</span></li>
<li>und <span class="math inline">\(L_{[x_1]}\in \mathbb R^{n_2\times (n_2\times n_1)}\)</span> die Ableitung von <span class="math inline">\(A_2x_1\)</span> nach <span class="math inline">\(A_2\)</span> (vgl. oben).</li>
</ul>
<p>Bezüglich <span class="math inline">\(A_1\)</span> ergibt die Kettenregel direkt
<span class="math display">\[\begin{equation*}
\partial_{A_1} = \sigma&#39;(A_2x_1)A_2\sigma&#39;(A_1x)L_{[x]}
\end{equation*}\]</span></p>
<div id="rekursion" class="section level3 hasAnchor" number="11.2.1">
<h3><span class="header-section-number">11.2.1</span> Rekursion<a href="implementierungen-anwendungsbeispiele-backpropagation.html#rekursion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Wir bemerken, dass die <span class="math inline">\(A_i\)</span> Komponenten des Gradienten von <span class="math inline">\(s\)</span> über
<span class="math display">\[\begin{equation*}
\delta_N := \sigma&#39;(A_Nx_{N-1}), \quad \partial_{A_N} s(x) = \delta_N L_{[x_{N-1}]}
\end{equation*}\]</span>
für die letzte <em>layer</em> und dann rekursiv über</p>
<p><span class="math display">\[\begin{equation*}
\delta_{n-1} := \delta_nA_n\sigma&#39;(A_{n-1}x_{n-1}), \quad \partial_{A_{n-1}} s(x) = \delta_{n-1} L_{[x_{n-2}]}
\end{equation*}\]</span>
von der letzten bis zur ersten <em>layer</em> berechnet werden können.</p>
</div>
</div>
<div id="praktische-berechnung-des-gradienten" class="section level2 hasAnchor" number="11.3">
<h2><span class="header-section-number">11.3</span> Praktische Berechnung des Gradienten<a href="implementierungen-anwendungsbeispiele-backpropagation.html#praktische-berechnung-des-gradienten" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="grad-vs-jac" class="JHSAYS">
<p>Im schönsten <em>machine learning</em>-Sprech sagt Jan hier immer Gradient obwohl
eher die Ableitung oder die Jacobi Matrix gemeint ist.</p>
</div>
<p>Mit <span class="math inline">\(\hat y = -2(y-s(x;p)) \in \mathbb R^{n_2\times 1}\)</span> bekommen wir aus obigen Formeln, dass
<span class="math display">\[\begin{equation*}
\begin{split}
\partial_{A_2}l((x,y); p) = \bigl \langle \hat y, \, \partial_{A_2}s(x;p)\bigr\rangle  &amp;= \hat y^T
\sigma&#39;(A_2x_1)L_{[x_1]} \\
&amp;= \tilde y^TL_{[x_1]},
\end{split}
\end{equation*}\]</span>
mit <span class="math inline">\(\tilde y=\sigma&#39;(A_2x_1)\hat y\)</span>, was wegen der Diagonalität von <span class="math inline">\(\sigma&#39;\)</span>
nur eine Skalierung der einzelnen Elemente von <span class="math inline">\(\hat y\)</span> darstellt.</p>
<p>Eine Betrachtung der Dimensionen <span class="math inline">\(\hat y \in \mathbb R^{n_2 \times 1}\)</span>,
<span class="math inline">\(\sigma&#39;(A_2x_1)\in \mathbb R^{n_2\times n_2}\)</span> und <span class="math inline">\(L_{[x_1]}\in \mathbb R^{n_2\times(n_2\times n_1)}\)</span> ergibt für das Produkt, dass
<span class="math display">\[\begin{equation*}
\tilde y^T L_{[x_1]} \in \mathbb R^{1\times (n_2 \times n_1)}
\end{equation*}\]</span>
sodass wir schon fast den Gradienten zum Parameter <span class="math inline">\(A_2 \in \mathbb R^{n_2 \times n_1}\)</span> addieren können.</p>
<p>Wir berechnen die Einträge <span class="math inline">\(g_{1ij}\)</span> des Gradienten <span class="math inline">\(\tilde y^T L_{[x_1]}\)</span>
als die <span class="math inline">\(1\)</span>-te (und einzige – da der Bildraum 1-dimensional ist) Komponente
(bzgl. die kanonischen Basis) auf die der <span class="math inline">\(ij\)</span> kanonische Basisvektor aus dem
Urbildraum abgebildet wird. Hier bietet sich als kanonische Basis
<span class="math inline">\(\{e^{(ij)}\}\subset \mathbb R^{n_2\times n_1}\)</span>
die Menge der Matrizen an, die <span class="math inline">\(0\)</span> sind bis auf eine <span class="math inline">\(1\)</span> in der <span class="math inline">\(i\)</span>-ten Zeile an der
<span class="math inline">\(j\)</span>-ten Stelle. In der Tat gilt dann
<span class="math display">\[\begin{equation*}
\mathbb R^{n_2\times n_1}\ni A = [a_{ij}] \leftrightarrow A =
\sum_{i,j}a_{ij}e^{(ij)}
\end{equation*}\]</span>
und weiter
<span class="math display">\[\begin{equation*}
g_{1ij}=\tilde y^TL_{[x_1]}e^{(ij)} = \tilde y^T e^{(ij)}x_1 = \tilde y_i (x_{1})_j
\end{equation*}\]</span>
sodass wir als <span class="math inline">\(1\)</span>-te und einzige (matrixwertige) Komponente des Gradienten das
äußere Produkt von <span class="math inline">\(\tilde y\)</span> und <span class="math inline">\(x_1\)</span> erhalten
<span class="math display">\[\begin{equation*}
(\tilde y^T L_{[x_1]})_1 = \tilde y x_1^T \in \mathbb R^{n_2 \times n_1}.
\end{equation*}\]</span></p>
<p>Für die Ableitung bezüglich <span class="math inline">\(A_k\)</span> in einem <span class="math inline">\(N\)</span>-layer Netzwerk bekommen
wir mit obiger Rekursionsformel
<span class="math display">\[\begin{equation*}
\partial_{A_k} l(x;p) = \hat y^T \delta_k L_{[x_{k-1}]} = \tilde{\tilde y}
x_{k-1}^T
\end{equation*}\]</span>
mit
<span class="math display">\[\begin{equation*}
{\tilde {\tilde y}}^T = \hat y^T \sigma&#39;(A_Nx_N)A_N\sigma&#39;(A_{N-1})A_{N-1}
\dotsm \sigma&#39;(A_kx_{k-1}).
\end{equation*}\]</span></p>
</div>
<div id="implementierungen-und-beispiele" class="section level2 hasAnchor" number="11.4">
<h2><span class="header-section-number">11.4</span> Implementierungen und Beispiele<a href="implementierungen-anwendungsbeispiele-backpropagation.html#implementierungen-und-beispiele" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Jan könnte als grobe Einschätzung sagen:</p>
<div id="ad-applications" class="JHSAYS">
<p>Die Eleganz und Effizienz von AD für Optimierungsanwendungen ist
unbestritten. Dennoch bedeutet der Einsatz eine Abwägung von
Extra-Implementierungsaufwand und Selbstbeschränkung in Struktur und
Funktionsumfang des Codes (das sind zwei Seiten der gleichen Medaille – auch
<em>nonstandard but clever</em> Funktionen können mit entsprechendem Aufwand
A-differenziert werden).
Deswegen ist AD in der Forschung (hier wird der Mehraufwand gescheut) und in der
industriellen Anwendung (viel legacy code mit spezifisch cleveren Funktionen)
eine Randerscheinung wenn auch mit leuchtenden Erfolgsgeschichten.</p>
</div>
<p>Sicherlich ist es am besten im Bezug auf Aufwand und Funtionalität, den
code sofort mit Augenmerk auf AD zu entwickeln.</p>
<p>Soll der AD <em>overhead</em> nicht selbst mitentwickelt werden, empfiehlt es sich auf
Programmumgebungen zurückzugreifen, die einen <em>AD framework</em> mitbringen.
Hier greift obige Abwägung – die AD verursacht kaum Mehraufwand allerdings
muss der eigene Code an die vorgegebene Struktur und Funktionalität
angepasst sein.</p>
<p>Für bestehenden Code gibt es zwei Herangehensweisen (neben der
“Um”entwicklung).</p>
<ol style="list-style-type: decimal">
<li><p><em>precompiling</em> – “AD ready” Code wird automatisiert aus bestehendem Code
generiert.</p></li>
<li><p><em>overloading</em> – Es werden AD Variablentypen und Klassen definiert und die
Grundoperationen <em>überladen</em> sodass mit minimalen Änderungen ein
bestehender Code die AD Funktionalität bekommt.</p></li>
</ol>
<div id="anwendungsbeispiele" class="section level3 hasAnchor" number="11.4.1">
<h3><span class="header-section-number">11.4.1</span> Anwendungsbeispiele<a href="implementierungen-anwendungsbeispiele-backpropagation.html#anwendungsbeispiele" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><a href="https://doi.org/10.1007/s11081-019-09474-x">Algorithmic differentiation of an industrial airfoil design tool coupled with the adjoint CFD method</a> – ein Beispiel in dem ein kompletter Simulationscode <em>automatisch differenziert</em> wurde um Tragflügel zu optimieren.</li>
</ul>
</div>
<div id="implementierungen" class="section level3 hasAnchor" number="11.4.2">
<h3><span class="header-section-number">11.4.2</span> Implementierungen<a href="implementierungen-anwendungsbeispiele-backpropagation.html#implementierungen" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ul>
<li><a href="https://web.casadi.org/">CASADI</a> – eine <em>AD aware</em> (für <code>python</code> und <code>Matlab/Octave</code>) Umgebung zur Optimierung (auch mit
Differentialgleichungen)</li>
<li><a href="https://pytorch.org/">PyTorch</a> – eine <em>machine learning toolbox</em> mit AD integriert zur
Gradientenberechnung</li>
<li><a href="https://www3.math.tu-berlin.de/Vorlesungen/SS06/AlgoDiff/adolc-110.pdf">ADOL-C</a> – ein <em>overloading</em> framework für <code>C</code> und <code>C++</code></li>
<li><a href="https://tapenade.gitlabpages.inria.fr/userdoc/build/html/tapenade/whatisad.html">Tapenade</a> – ein <em>precompiler</em> der aus Code <em>AD Code</em> generiert</li>
</ul>
</div>
</div>
<div id="aufgaben-3" class="section level2 hasAnchor" number="11.5">
<h2><span class="header-section-number">11.5</span> Aufgaben<a href="implementierungen-anwendungsbeispiele-backpropagation.html#aufgaben-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="ableitung-und-newton-der-riccati-gleichung" class="section level3 hasAnchor" number="11.5.1">
<h3><span class="header-section-number">11.5.1</span> Ableitung und Newton der Riccati Gleichung<a href="implementierungen-anwendungsbeispiele-backpropagation.html#ableitung-und-newton-der-riccati-gleichung" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Weisen Sie nach, dass der Gradient der (matrixwertigen) Riccati Abbildung
<a href="implementierungen-anwendungsbeispiele-backpropagation.html#eq:eqn-ric-operator">(11.1)</a>
tatsächlich die gegebene Form hat und formulieren Sie damit ein Newton
Verfahren zur Lösung der Riccati Gleichung <span class="math inline">\(f(S)=0\)</span>.</p>
</div>
<div id="ableitung-von-ax-nach-der-matrix" class="section level3 hasAnchor" number="11.5.2">
<h3><span class="header-section-number">11.5.2</span> Ableitung von <span class="math inline">\(Ax\)</span> nach der Matrix<a href="implementierungen-anwendungsbeispiele-backpropagation.html#ableitung-von-ax-nach-der-matrix" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Sei <span class="math inline">\(A=[a_{ij}]\in \mathbb R^{m\times n}\)</span>. Bestimmen Sie partielle Ableitung der
Funktion <span class="math inline">\(f_x(A)=A(x)\)</span> nach einer Komponente
<span class="math display">\[\begin{equation*}
\frac{\partial {f_x}}{{\partial a_{ij}}}(A)
\end{equation*}\]</span>
und geben Sie davon ausgehend eine Darstellung der totalen Ableitung <span class="math inline">\(\partial f_x(A) \colon \mathbb R^{m\times n}\to \mathbb R^{m}\)</span> an</p>
</div>
<div id="backward-propagation" class="section level3 hasAnchor" number="11.5.3">
<h3><span class="header-section-number">11.5.3</span> Backward Propagation<a href="implementierungen-anwendungsbeispiele-backpropagation.html#backward-propagation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Leiten Sie aus <span class="math inline">\(\mathbf A = \begin{bmatrix} A &amp; b \\ 0 &amp; 1 \end{bmatrix}\)</span> und
<span class="math inline">\(\mathbf x = (x, 1)\)</span> sowie <span class="math inline">\(\mathbf y = (y, 1)\)</span>
an der Stelle <span class="math inline">\([A\,b]=[A_0\, b_0]\)</span> die Formel für den Gradienten
<span class="math display">\[\begin{equation*}
\partial_{[A\,b]} (\frac 12 \|\mathbf y-\sigma(\mathbf A\mathbf x)\|_2^2)\Bigr|_{[A_0\, b_0]}=\Bigl (\sigma&#39;(A_0x+b_0)\bigr (y-\sigma(A_0x+b_0)\bigr )\Bigr)\,\mathbf x^T
\end{equation*}\]</span>
Bestätigen Sie sie numerisch, beispielsweise mit Hilfe der Code snippets aus
<a href="ein-nn-beispiel.html#beispiel-implementierung">Beispiel Implementierung</a>,
für ein Beispiel Netzwerk <span class="math inline">\(s\colon \mathbb R^{3}\to \mathbb R^{2}\colon \xi \to \sigma(\tilde A\xi + b)\)</span>
mit <span class="math inline">\(\sigma = \tanh\)</span> an der Stelle
<span class="math inline">\(A_0=\begin{bmatrix}2 &amp; 1 &amp; 1 \\ 1 &amp; 2&amp; 0 \end{bmatrix}\)</span> und
<span class="math inline">\(b_0=\begin{bmatrix}0\\1 \end{bmatrix}\)</span> beim Datenpunkt <span class="math inline">\((\tilde x, y) = ((1,1,1), (1, 1))\)</span>.</p>
</div>
<div id="ad-and-autograd-in-pytorch" class="section level3 hasAnchor" number="11.5.4">
<h3><span class="header-section-number">11.5.4</span> AD and Autograd in <code>pytorch</code><a href="implementierungen-anwendungsbeispiele-backpropagation.html#ad-and-autograd-in-pytorch" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Arbeiten Sie sie sich durch die <code>pytorch</code> tutorials</p>
<ol style="list-style-type: decimal">
<li><a href="https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html">Introduction to <code>torch.autograd</code></a></li>
<li><a href="https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html">AD with <code>torch.autograd</code></a></li>
</ol>
<p>und erweitern Sie die Codes um den Gradienten (in Bezug auf <span class="math inline">\(A\in \mathbb R^{2\times 2}\)</span>) von <span class="math inline">\(x\to x^TAx\)</span> für <span class="math inline">\(x\in \mathbb R^{2}\)</span> an der Stelle
<span class="math inline">\(A_0=\begin{bmatrix}2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix}\)</span> zu bestimmen.</p>

</div>
</div>
</div>
<h3>Referenzen<a href="referenzen.html#referenzen" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references">
<div id="ref-KurZ05">
<p> Kurdila, A.J., Zabarankin, M.: Convex functional analysis. Birkhäuser (2005)</p>
</div>
<div id="ref-Tr09">
<p> Tröltzsch, F.: Optimale steuerung partieller differentialgleichungen. Vieweg+Teubner, Wiesbaden, Germany (2009)</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="automatisches-algorithmisches-differenzieren.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="nachklapp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["NdML.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
