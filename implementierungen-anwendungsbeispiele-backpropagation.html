<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11 Implementierungen, Anwendungsbeispiele, Backpropagation | Numerik des Maschinellen Lernens</title>
  <meta name="description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2024" />
  <meta name="generator" content="bookdown 0.39 and GitBook 2.6.7" />

  <meta property="og:title" content="11 Implementierungen, Anwendungsbeispiele, Backpropagation | Numerik des Maschinellen Lernens" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2024" />
  <meta name="github-repo" content="highlando/script-ndml" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11 Implementierungen, Anwendungsbeispiele, Backpropagation | Numerik des Maschinellen Lernens" />
  
  <meta name="twitter:description" content="Vorlesungsnotizen zu meiner integrierten Vorlesung im SoSe 2024" />
  

<meta name="author" content="Jan Heiland" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="automatisches-algorithmisches-differenzieren.html"/>
<link rel="next" href="nachklapp.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">NdML</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Vorwort</a></li>
<li class="chapter" data-level="1" data-path="einführung.html"><a href="einführung.html"><i class="fa fa-check"></i><b>1</b> Einführung</a>
<ul>
<li class="chapter" data-level="1.1" data-path="einführung.html"><a href="einführung.html#was-ist-ein-algorithmus"><i class="fa fa-check"></i><b>1.1</b> Was ist ein Algorithmus</a></li>
<li class="chapter" data-level="1.2" data-path="einführung.html"><a href="einführung.html#konsistenz-stabilität-genauigkeit"><i class="fa fa-check"></i><b>1.2</b> Konsistenz, Stabilität, Genauigkeit</a></li>
<li class="chapter" data-level="1.3" data-path="einführung.html"><a href="einführung.html#rechenkomplexität"><i class="fa fa-check"></i><b>1.3</b> Rechenkomplexität</a></li>
<li class="chapter" data-level="1.4" data-path="einführung.html"><a href="einführung.html#literatur"><i class="fa fa-check"></i><b>1.4</b> Literatur</a></li>
<li class="chapter" data-level="1.5" data-path="einführung.html"><a href="einführung.html#übungen"><i class="fa fa-check"></i><b>1.5</b> Übungen</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html"><i class="fa fa-check"></i><b>2</b> Fehler und Konditionierung</a>
<ul>
<li class="chapter" data-level="2.1" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html#fehler"><i class="fa fa-check"></i><b>2.1</b> Fehler</a></li>
<li class="chapter" data-level="2.2" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html#kondition"><i class="fa fa-check"></i><b>2.2</b> Kondition</a></li>
<li class="chapter" data-level="2.3" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html#kondition-der-grundrechenarten"><i class="fa fa-check"></i><b>2.3</b> Kondition der Grundrechenarten</a></li>
<li class="chapter" data-level="2.4" data-path="fehler-und-konditionierung.html"><a href="fehler-und-konditionierung.html#übungen-1"><i class="fa fa-check"></i><b>2.4</b> Übungen</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="iterative-methoden.html"><a href="iterative-methoden.html"><i class="fa fa-check"></i><b>3</b> Iterative Methoden</a>
<ul>
<li class="chapter" data-level="3.1" data-path="iterative-methoden.html"><a href="iterative-methoden.html#iterative-methoden-als-fixpunktiteration"><i class="fa fa-check"></i><b>3.1</b> Iterative Methoden als Fixpunktiteration</a></li>
<li class="chapter" data-level="3.2" data-path="iterative-methoden.html"><a href="iterative-methoden.html#gradientenabstiegsverfahren"><i class="fa fa-check"></i><b>3.2</b> Gradientenabstiegsverfahren</a></li>
<li class="chapter" data-level="3.3" data-path="iterative-methoden.html"><a href="iterative-methoden.html#auxiliary-function-methods"><i class="fa fa-check"></i><b>3.3</b> Auxiliary Function Methods</a></li>
<li class="chapter" data-level="3.4" data-path="iterative-methoden.html"><a href="iterative-methoden.html#übungen-2"><i class="fa fa-check"></i><b>3.4</b> Übungen</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html"><i class="fa fa-check"></i><b>4</b> Stochastisches Gradientenverfahren</a>
<ul>
<li class="chapter" data-level="4.1" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html#motivation-und-algorithmus"><i class="fa fa-check"></i><b>4.1</b> Motivation und Algorithmus</a></li>
<li class="chapter" data-level="4.2" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html#iterative_method"><i class="fa fa-check"></i><b>4.2</b> Stochastisches Abstiegsverfahren</a></li>
<li class="chapter" data-level="4.3" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html#konvergenzanalyse"><i class="fa fa-check"></i><b>4.3</b> Konvergenzanalyse</a></li>
<li class="chapter" data-level="4.4" data-path="stochastisches-gradientenverfahren.html"><a href="stochastisches-gradientenverfahren.html#übungen-3"><i class="fa fa-check"></i><b>4.4</b> Übungen</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="ein-nn-beispiel.html"><a href="ein-nn-beispiel.html"><i class="fa fa-check"></i><b>5</b> Ein NN Beispiel</a>
<ul>
<li class="chapter" data-level="5.1" data-path="ein-nn-beispiel.html"><a href="ein-nn-beispiel.html#der-penguins-datensatz"><i class="fa fa-check"></i><b>5.1</b> Der PENGUINS Datensatz</a></li>
<li class="chapter" data-level="5.2" data-path="ein-nn-beispiel.html"><a href="ein-nn-beispiel.html#ein-2-layer-neuronales-netz-zur-klassifizierung"><i class="fa fa-check"></i><b>5.2</b> Ein <em>2</em>-Layer Neuronales Netz zur Klassifizierung</a></li>
<li class="chapter" data-level="5.3" data-path="ein-nn-beispiel.html"><a href="ein-nn-beispiel.html#beispiel-implementierung"><i class="fa fa-check"></i><b>5.3</b> Beispiel Implementierung</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="singulärwert-zerlegung.html"><a href="singulärwert-zerlegung.html"><i class="fa fa-check"></i><b>6</b> Singulärwert Zerlegung</a>
<ul>
<li class="chapter" data-level="6.1" data-path="singulärwert-zerlegung.html"><a href="singulärwert-zerlegung.html#definition-und-eigenschaften"><i class="fa fa-check"></i><b>6.1</b> Definition und Eigenschaften</a></li>
<li class="chapter" data-level="6.2" data-path="singulärwert-zerlegung.html"><a href="singulärwert-zerlegung.html#numerische-berechnung"><i class="fa fa-check"></i><b>6.2</b> Numerische Berechnung</a></li>
<li class="chapter" data-level="6.3" data-path="singulärwert-zerlegung.html"><a href="singulärwert-zerlegung.html#aufgaben"><i class="fa fa-check"></i><b>6.3</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="pca-und-weitere-svd-anwendungen.html"><a href="pca-und-weitere-svd-anwendungen.html"><i class="fa fa-check"></i><b>7</b> PCA und weitere SVD Anwendungen</a>
<ul>
<li class="chapter" data-level="7.1" data-path="pca-und-weitere-svd-anwendungen.html"><a href="pca-und-weitere-svd-anwendungen.html#proper-orthogonal-decomposition-pod"><i class="fa fa-check"></i><b>7.1</b> Proper-Orthogonal Decomposition – POD</a></li>
<li class="chapter" data-level="7.2" data-path="pca-und-weitere-svd-anwendungen.html"><a href="pca-und-weitere-svd-anwendungen.html#simultane-diagonalisierung"><i class="fa fa-check"></i><b>7.2</b> Simultane Diagonalisierung</a></li>
<li class="chapter" data-level="7.3" data-path="pca-und-weitere-svd-anwendungen.html"><a href="pca-und-weitere-svd-anwendungen.html#pca"><i class="fa fa-check"></i><b>7.3</b> PCA</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="support-vector-machines.html"><a href="support-vector-machines.html"><i class="fa fa-check"></i><b>8</b> Support Vector Machines</a>
<ul>
<li class="chapter" data-level="8.1" data-path="support-vector-machines.html"><a href="support-vector-machines.html#problemstellung"><i class="fa fa-check"></i><b>8.1</b> Problemstellung</a></li>
<li class="chapter" data-level="8.2" data-path="support-vector-machines.html"><a href="support-vector-machines.html#maximierung-des-minimalen-abstands"><i class="fa fa-check"></i><b>8.2</b> Maximierung des Minimalen Abstands</a></li>
<li class="chapter" data-level="8.3" data-path="support-vector-machines.html"><a href="support-vector-machines.html#aufgaben-1"><i class="fa fa-check"></i><b>8.3</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="best-and-universal-approximation.html"><a href="best-and-universal-approximation.html"><i class="fa fa-check"></i><b>9</b> Best and Universal Approximation</a>
<ul>
<li class="chapter" data-level="9.1" data-path="best-and-universal-approximation.html"><a href="best-and-universal-approximation.html#universal-approximation"><i class="fa fa-check"></i><b>9.1</b> Universal Approximation</a></li>
<li class="chapter" data-level="9.2" data-path="best-and-universal-approximation.html"><a href="best-and-universal-approximation.html#aufgaben-2"><i class="fa fa-check"></i><b>9.2</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html"><i class="fa fa-check"></i><b>10</b> Automatisches (Algorithmisches) Differenzieren</a>
<ul>
<li class="chapter" data-level="10.1" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#andere-differentiationsmethoden"><i class="fa fa-check"></i><b>10.1</b> Andere Differentiationsmethoden</a></li>
<li class="chapter" data-level="10.2" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#anwendungen"><i class="fa fa-check"></i><b>10.2</b> Anwendungen</a></li>
<li class="chapter" data-level="10.3" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#vorwärts--und-rückwärtsakkumulation"><i class="fa fa-check"></i><b>10.3</b> Vorwärts- und Rückwärtsakkumulation</a></li>
<li class="chapter" data-level="10.4" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#ad-vorwärtsmodus"><i class="fa fa-check"></i><b>10.4</b> AD – Vorwärtsmodus</a></li>
<li class="chapter" data-level="10.5" data-path="automatisches-algorithmisches-differenzieren.html"><a href="automatisches-algorithmisches-differenzieren.html#rückwärtsmodus"><i class="fa fa-check"></i><b>10.5</b> Rückwärtsmodus</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html"><i class="fa fa-check"></i><b>11</b> Implementierungen, Anwendungsbeispiele, Backpropagation</a>
<ul>
<li class="chapter" data-level="11.1" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#exkurs-gradienten-und-repräsentation"><i class="fa fa-check"></i><b>11.1</b> Exkurs – Gradienten und Repräsentation</a></li>
<li class="chapter" data-level="11.2" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#backpropagation"><i class="fa fa-check"></i><b>11.2</b> Backpropagation</a></li>
<li class="chapter" data-level="11.3" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#praktische-berechnung-des-gradienten"><i class="fa fa-check"></i><b>11.3</b> Praktische Berechnung des Gradienten</a></li>
<li class="chapter" data-level="11.4" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#implementierungsbeispiele"><i class="fa fa-check"></i><b>11.4</b> Implementierungsbeispiele</a></li>
<li class="chapter" data-level="11.5" data-path="implementierungen-anwendungsbeispiele-backpropagation.html"><a href="implementierungen-anwendungsbeispiele-backpropagation.html#aufgaben-3"><i class="fa fa-check"></i><b>11.5</b> Aufgaben</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="nachklapp.html"><a href="nachklapp.html"><i class="fa fa-check"></i><b>12</b> Nachklapp</a></li>
<li class="chapter" data-level="" data-path="referenzen.html"><a href="referenzen.html"><i class="fa fa-check"></i>Referenzen</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Numerik des Maschinellen Lernens</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="implementierungen-anwendungsbeispiele-backpropagation" class="section level1 hasAnchor" number="11">
<h1><span class="header-section-number">11</span> Implementierungen, Anwendungsbeispiele, Backpropagation<a href="implementierungen-anwendungsbeispiele-backpropagation.html#implementierungen-anwendungsbeispiele-backpropagation" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="exkurs-gradienten-und-repräsentation" class="section level2 hasAnchor" number="11.1">
<h2><span class="header-section-number">11.1</span> Exkurs – Gradienten und Repräsentation<a href="implementierungen-anwendungsbeispiele-backpropagation.html#exkurs-gradienten-und-repräsentation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Die Berechnung von Gradienten ist ebenso wesentlich wie schwierig in numerischen
Algorithmen.
Viele <em>erfolgreiche</em> Algorithmen basieren auf effizient berechenbaren
Darstellungen oder Approximationen des Gradienten.</p>
<p>Ein erstes Beispiel ist der stochastische Gradientenabstieg, der auf einen
Schätzer statt des eigentlichen Gradienten baut.</p>
<p>In der Optimierung mit (partiellen) Differentialgleichungen wie
<span class="math display">\[\begin{equation*}
\mathcal J(u) = \frac 12 \int_0^T\|x(t)-x^*\|^2 + \|u(t)\|^2\,\mathsf{d}s\to \min_{u} \quad \text{s.t. }\dot x(t) =
Ax(t) + Bu(t)
\end{equation*}\]</span>
macht es einen entscheidenden Unterschied, dass der Gradient
<span class="math display">\[\partial_u \mathcal J(u) = u + B^Tp \]</span>
über die Lösung der adjungierten Gleichung
<span class="math display">\[\begin{equation*}
-\dot p(t) = A^T p(t) + x(t)-x^*, \quad p(T) = 0
\end{equation*}\]</span>
definiert ist (siehe beispielsweise (Kurdila/Zarabankin, Thm 5.5.1) für ein
abstraktes Resultat und bspw. (Troeltzsch Abschnitt 5.9.1) für eine
Umsetzung in einem Gradientenabstiegsverfahren mit PDEs).</p>
<p>Während diese Resultate in der Theorie die Wohlgestelltheit der
Optimierungsprobleme helfen zu analysieren, werden Sie in der Praxis gerne
verwendet weil die Lösung einer Differentialgleichung einfacher umzusetzen
ist (und im Zweifel auch effizienter) als die Berechnung von sehr abstrakten
Gradienten.</p>
<p>Um abstraktere Gradienten zu charakterisieren ist oftmals die Definition der
<em>totalen Ableitung</em> einer Funktion <span class="math inline">\(f\colon X\to Y\)</span> bei einem <span class="math inline">\(x\in X\)</span> als die lineare Abbildung <span class="math inline">\(L(x)\colon X\to Y\)</span> derart, dass
<span class="math display">\[\begin{equation*}
f(x+h) - f(x) - L(x)[h] = o(\| h\|)
\end{equation*}\]</span>
für <span class="math inline">\(h\in X\)</span> gilt.</p>
<p>Seien beispielsweise die Räume als <span class="math inline">\(X=Y=\mathbb R^{n\times n}\)</span> gegeben und
<span class="math display">\[\begin{equation*}
f(S) = A^TS + SA -SRS + Q
\end{equation*}\]</span>
dann hat der <em>Riccati</em> Operator <span class="math inline">\(f\)</span> mit Koeffizienten <span class="math inline">\(A\)</span>, <span class="math inline">\(R\)</span>, <span class="math inline">\(Q \in \mathbb R^{n\times n}\)</span>
demnach die Realisierung des Gradienten (an der Stelle <span class="math inline">\(S_0\)</span>) gegeben als
<span class="math display">\[\begin{equation*}
L(S_0)[h] = (A^T-S_0R)h + h(A-RS_0).
\end{equation*}\]</span></p>
<p>Für neuronale Netze ist der Gradient von <span class="math inline">\(f(x; \tilde A, b)=\tilde Ax+b\)</span> bezüglich der
<em>Gewichte</em> <span class="math inline">\(\tilde A\in \mathbb R^{m\times n-1}\)</span> und <span class="math inline">\(b\in \mathbb R^{m}\)</span> interessant.
Zunächst mal verstehen wir die affine lineare Abbildung im
projektiven Raum vermöge
<span class="math display">\[\begin{equation*}
y = Ax + b \longleftrightarrow \begin{bmatrix} y \\ 1 \end{bmatrix} = \begin{bmatrix}A &amp; b \\ 0 &amp; 1 \end{bmatrix}\begin{bmatrix} x \\ 1
\end{bmatrix}
\end{equation*}\]</span>
und betrachten einfach <span class="math inline">\(f(x; A)=Ax\)</span> mit <span class="math inline">\(A\in \mathbb R^{m\times n}\)</span>. Gemäß der Ableitungsformel gilt
<span class="math display">\[\begin{equation*}
f(x;A+h) - f(x;A) - hx = (A+h)x - Ax - hx = 0 = o(\|h\|)
\end{equation*}\]</span>
also im Prinzip ist <span class="math inline">\(L(A)\leftrightarrow x\)</span> die Ableitung wenn auch nicht als Vektor sondern als
<span class="math display">\[\begin{equation*}
L_{[x]}\colon \mathbb R^{n\times m} \to \mathbb R^{n}\colon h\to L_{[x]} h = hx.
\end{equation*}\]</span></p>
<div id="Lx-not-LA" class="JHSAYS">
<p>Weil hier an der Stelle <span class="math inline">\(A\)</span> abgeleitet wird, sollte hier <span class="math inline">\(L(A)\)</span> stehen. Da bei
linearen Abbildung die Ableitung überall gleich ist, gilt <span class="math inline">\(L(A)\equiv L\)</span>
unabhängig von <span class="math inline">\(A\)</span>. Für später wird es interessant sein, dass
die Ableitung direkt mit <span class="math inline">\(x\)</span> zusammenhängt, sodass wir das <span class="math inline">\(x\)</span> in die
Definition mit aufnehmen.</p>
</div>
<p>Diese Abbildung <span class="math inline">\(L_{[x]}\)</span> ist linear und könnte als Matrix (bzw. Tensor im Sinne
einer höherdimensionalen Datenstruktur)
im <span class="math inline">\(\mathbb R^{n \times (n\times m)}\)</span> realisiert werden.</p>
</div>
<div id="backpropagation" class="section level2 hasAnchor" number="11.2">
<h2><span class="header-section-number">11.2</span> Backpropagation<a href="implementierungen-anwendungsbeispiele-backpropagation.html#backpropagation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Die besondere Architektur neuronaler Netze, wie sie im Machine Learning
verwendet werden, impliziert eine besondere Struktur in der
Gradientenberechnung.</p>
<p>Wir betrachten ein 2-layer Netzwerk
<span class="math display">\[\begin{equation*}
s(x) = \sigma(A_2 \sigma(A_1x))
\end{equation*}\]</span>
mit den <em>bias</em>-Vektoren <span class="math inline">\(b_1\)</span> und <span class="math inline">\(b_2\)</span> wie oben erläutert bereits in den Matrizen <span class="math inline">\(A_1\in \mathbb R^{n_0\times n_1}\)</span> und <span class="math inline">\(A_2 \in \mathbb R^{n_2\times n_1}\)</span> integriert.
Für das <em>learning</em> wird zu Datenpunkten <span class="math inline">\((x, y) \in \mathbb R^{n_0}\times \mathbb R^{n_2}\)</span> die Ableitung der Funktion
<span class="math display">\[\begin{equation*}
(A_1, A_2) \to l(x; A_1, A_2) = \|y-s(x)\|_2^2
\end{equation*}\]</span>
bezüglich des Parameter“vektors” <span class="math inline">\(p=(A_2, A_1)\)</span> gesucht.</p>
<p>Eine erste Anwendung der Kettenregel ergibt
<span class="math display">\[\begin{equation*}
\frac{\partial}{\partial p}l(x; p) = -2\bigl \langle y-s(x;p), \, \partial_p s(x;p)\bigr\rangle 
\end{equation*}\]</span>
sodass wir uns erstmal um die Ableitung von <span class="math inline">\(s\)</span> bezüglich <span class="math inline">\(p\)</span> kümmern
können.</p>
<p>Bezüglich <span class="math inline">\(A_2\)</span> ergibt die Kettenregel direkt
<span class="math display">\[\begin{equation*}
\frac{\partial s}{\partial A_2} = \sigma&#39;(A_2x_1)L_{[x_1]}
\end{equation*}\]</span>
wobei</p>
<ul>
<li><span class="math inline">\(x_1:=\sigma(A_1x)\)</span> der Zwischenwert nach der vorletzten (hier der ersten) <em>layer</em> ist,</li>
<li><span class="math inline">\(\sigma&#39;(A_2x_1)\in \mathbb R^{n_2&#39;\times n_2}\)</span> die (diagonale) Jacobimatrix der komponentenweise definierten Aktivierungsfunktion <span class="math inline">\(\sigma\)</span></li>
<li>und <span class="math inline">\(L_{[x_1]}\in \mathbb R^{n_2\times (n_2\times n_1)}\)</span> die Ableitung von <span class="math inline">\(A_2x_1\)</span> nach <span class="math inline">\(A_2\)</span> (vgl. oben).</li>
</ul>
<p>Bezüglich <span class="math inline">\(A_1\)</span> ergibt die Kettenregel direkt
<span class="math display">\[\begin{equation*}
\frac{\partial s}{\partial A_1} = \sigma&#39;(A_2x_1)A_2\sigma&#39;(A_1x)L_{[x]}
\end{equation*}\]</span></p>
<div id="rekursion" class="section level3 hasAnchor" number="11.2.1">
<h3><span class="header-section-number">11.2.1</span> Rekursion<a href="implementierungen-anwendungsbeispiele-backpropagation.html#rekursion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Wir bemerken, dass die <span class="math inline">\(A_i\)</span> Komponenten des Gradienten von <span class="math inline">\(s\)</span> über
<span class="math display">\[\begin{equation*}
\delta_N := \sigma&#39;(A_Nx_{N-1}), \quad \partial_{A_N} s(x) = \delta_N L_{[x_{N-1}]}
\end{equation*}\]</span>
für die letzte <em>layer</em> und dann rekursiv über</p>
<p><span class="math display">\[\begin{equation*}
\delta_{n-1} := \delta_n\sigma&#39;(A_{n-1}x_{n-1}), \quad \partial_{A_{n-1}} s(x) = \delta_{n-1} L_{[x_{n-2}]}
\end{equation*}\]</span>
von der letzten bis zur ersten <em>layer</em> berechnet werden können.</p>
</div>
</div>
<div id="praktische-berechnung-des-gradienten" class="section level2 hasAnchor" number="11.3">
<h2><span class="header-section-number">11.3</span> Praktische Berechnung des Gradienten<a href="implementierungen-anwendungsbeispiele-backpropagation.html#praktische-berechnung-des-gradienten" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="grad-vs-jac" class="JHSAYS">
<p>Im schönsten <em>machine learning</em>-Sprech sagt Jan hier immer Gradient obwohl
eher die Ableitung oder die Jacobi Matrix gemeint ist.</p>
</div>
<p>Mit <span class="math inline">\(\hat y = -2(y-s(x;p)) \in \mathbb R^{n_2\times 1}\)</span> bekommen wir aus obigen Formeln, dass
<span class="math display">\[\begin{equation*}
\begin{split}
\partial_{A_2}l(x; p) = \bigl \langle \hat y, \, \partial_{A_2}s(x;p)\bigr\rangle  &amp;= \hat y^T
\sigma&#39;(A_2x_1)L_{[x_1]} \\
&amp;= \tilde y^TL_{[x_1]}.
\end{split}
\end{equation*}\]</span></p>
<p>Eine Betrachtung der Dimensionen <span class="math inline">\(\hat y \in \mathbb R^{n_2 \times 1}\)</span>,
<span class="math inline">\(\sigma&#39;(A_2x_1)\in \mathbb R^{n_2\times n_2}\)</span> und <span class="math inline">\(L_{[x_1]}\in \mathbb R^{n_2\times(n_2\times n_1)}\)</span> ergibt für das Produkt, dass
<span class="math display">\[\begin{equation*}
\tilde y^T L_{[x_1]} \in \mathbb R^{1\times (n_2 \times n_1)}
\end{equation*}\]</span>
sodass wir schon fast den Gradienten zum Parameter <span class="math inline">\(A_2 \in \mathbb R^{n_2 \times n_1}\)</span> addieren können.</p>
<p>Wir berechnen die Einträge <span class="math inline">\(g_{1ij}\)</span> des Gradienten <span class="math inline">\(\tilde y^T L_{[x_1]}\)</span>
als die <span class="math inline">\(1\)</span>-te (und einzige – da der Bildraum 1-dimensional ist) Komponente
(bzgl. die kanonischen Basis) auf die der <span class="math inline">\(ij\)</span> kanonische Basisvektor aus dem
Urbildraum abgebildet wird. Hier bietet sich als kanonische Basis <span class="math inline">\(\{e^{(ij)}\}\)</span>
die Menge der Matrizen an, die <span class="math inline">\(0\)</span> sind bis auf eine <span class="math inline">\(1\)</span> in der <span class="math inline">\(i\)</span>-ten Zeile an der
<span class="math inline">\(j\)</span>-ten Stelle. In der Tat gilt dann
<span class="math display">\[\begin{equation*}
\mathbb R^{n_2\times n_1}\ni A = [a_{ij}] \leftrightarrow A =
\sum_{i,j}a_{ij}e^{(ij)}
\end{equation*}\]</span>
Damit gilt
<span class="math display">\[\begin{equation*}
g_{1ij}=\tilde y^TL_{[x_1]}e^{(ij)} = \tilde y^T e_{ij}x_1 = \tilde y_i (x_{1})_j
\end{equation*}\]</span>
sodass wir als <span class="math inline">\(1\)</span>-te und einzige (matrixwertige) Komponente des Gradienten das
äußere Produkt von <span class="math inline">\(\tilde y\)</span> und <span class="math inline">\(x_1\)</span> erhalten
<span class="math display">\[\begin{equation*}
(\tilde y^T L_{[x_1]})_1 = \tilde y x_1^T \in \mathbb R^{n_2 \times n_1}.
\end{equation*}\]</span></p>
</div>
<div id="implementierungsbeispiele" class="section level2 hasAnchor" number="11.4">
<h2><span class="header-section-number">11.4</span> Implementierungsbeispiele<a href="implementierungen-anwendungsbeispiele-backpropagation.html#implementierungsbeispiele" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ul>
<li>Beispiel in PyTorch</li>
<li>CASADI</li>
<li>Nico Gauger</li>
</ul>
</div>
<div id="aufgaben-3" class="section level2 hasAnchor" number="11.5">
<h2><span class="header-section-number">11.5</span> Aufgaben<a href="implementierungen-anwendungsbeispiele-backpropagation.html#aufgaben-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="ableitung-und-newton" class="section level3 hasAnchor" number="11.5.1">
<h3><span class="header-section-number">11.5.1</span> Ableitung und Newton<a href="implementierungen-anwendungsbeispiele-backpropagation.html#ableitung-und-newton" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Weisen Sie nach, dass der Gradient der (matrixwertigen) Riccati Abbildung
tatsächlich die gegebene Form hat und formulieren Sie damit ein Newton
Verfahren zur Lösung der Riccati Gleichung <span class="math inline">\(f(S)=0\)</span>.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="automatisches-algorithmisches-differenzieren.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="nachklapp.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["NdML.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
